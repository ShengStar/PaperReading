# 论文阅读
## 2022年3月29日
### VISTA: Boosting 3D Object Detection via Dual Cross-VIew SpaTial Attention
```
```
#### 背景
由于激光雷达点云的稀疏性（sparsity）和不规则性（irregularity），目前仍未实现精确可靠的3D检测。
#### 创新点
- 解耦分类任务和回归任务，并使用额外的注意力约束，使注意模块能够专注于特定的目标，而不是一般的点。
- 为了提高三维目标检测的性能，提出通过双视角空间关注(Dual CrossVIew spatial Attention, VISTA)从全局空间背景中提取高质量的融合多视角特征，用于建议预测。提出的VISTA利用了源自Transformer的注意力机制，该机制已成功应用于各种研究环境(如自然语言处理、2D计算机视觉)。与直接通过坐标投影进行融合相比，VISTA中内置的注意机制利用全局信息，通过将单个视图的特征作为特征元素序列来自适应地建模视图之间的所有两两相关性。
- 为了全面地建模交叉视图相关性，必须考虑两个视图中的局部上下文，因此用卷积算子代替传统注意模块中的MLPs，我们在第6节中展示了卷积算子的有效性。然而，学习视图之间的相关性仍然具有挑战性，如第6节所示。直接采用注意机制进行多视角融合收效甚微，这主要是由于任务三维目标检测本身的特点所致。
#### 动机
- 不同的视图有各自的优点和缺点。在BEV中，对象之间不重叠，并且每个对象的大小与自我载体的距离无关。RV是LiDAR点云的原生表示，因此可以产生紧凑、密集的特征。然而，无论选择BEV还是RV，投影都会不可避免地损害三维空间中所传递空间信息的完整性。BEV表示非常稀疏，它整合了三维点云的高度信息，在RV中由于失去深度信息，遮挡和物体大小变化会更加严重。显然，多视角联合学习，即多视角融合，为我们提供了一种精确的三维目标检测的解决方案。（具体略）
- 一般来说，三维物体检测任务可分为分类和回归两个子任务。三维目标检测器在整个三维场景中检测目标时面临许多挑战，如遮挡、背景噪声和点云的稀缺纹理信息。因此，注意相关性很难学习，注意模块倾向于学习整个场景的均值，这是意料之外的，因为注意模块是为了关注感兴趣的区域而设计的。因此，我们明确地约束了注意力机制学习到的注意力地图的变化，从而引导注意力模块意识到复杂的三维室外场景中的有意义区域。
- 此外，分类和回归的不同学习目标决定了在注意模块中学习到的查询和键值的不同期望。跨不同对象的各种回归目标(例如，缩放、翻译)期望查询和键知道对象的特征。相反，分类任务推动网络理解对象类的公共属性。不可避免的是，共享同样的注意力建模将会给这两个任务的训练带来冲突。此外，一方面，由于纹理信息的丢失，神经网络难以从点云中提取语义特征。另一方面，神经网络可以很容易地从点云中学习物体的几何特性。因此，在训练过程中，出现了以回归方法为主的分类方法的困境。为了应对这些挑战，我们将这两个任务在VISTA中解耦，以学习根据不同的任务来聚合不同的线索。
- 由于三维目标检测任务本身的特点和室外三维点云的固有特性，现有的注意模块无法聚焦于场景中感兴趣的区域。而且，网络训练很容易被回归任务所支配。而提出的VISTA则通过解耦的注意力建模和设计的训练约束来解决上述问题，从而能够产生高质量的融合多视图特征用于三维目标检测。
#### 不足

#### 三句话总结
VISTA融合BEV视角和RV视角，并且把3D检测任务解藕为分类任务和回归任务，将注意力变异约束应用于VISTA，促进注意力学习，并赋予网络关注感兴趣区域的能力。

### TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers

#### 背景
激光雷达和相机是自动驾驶三维目标检测的两个重要传感器。尽管传感器融合在这一领域越来越受欢迎，但其对不良图像条件(如照明不良和传感器错位（misalignment）)的鲁棒性尚未得到充分研究。
现有的融合方法容易受到这些条件的影响，主要是由于激光雷达点与图像像素之间通过标定矩阵建立的硬关联。
我们提出TransFusion，一个强大的解决方案，以LiDARcamera融合与软关联机制，以处理低劣的图像条件。
具体来说，由卷积骨干和基于Transformer解码器的检测头组成。
解码器的第一层使用稀疏的对象查询集预测来自LiDAR点云的初始边界框，
其第二层解码器自适应地将对象查询与有用的图像特征融合在一起，利用空间和上下文关系。
Transformer的注意力机制使我们的模型能够自适应地决定从图像中获取什么信息和什么位置，从而形成一个鲁棒和有效的融合策略。
此外，还设计了一种图像引导的查询初始化策略来处理点云中难以检测的对象。TransFusion在大规模数据集上实现了最先进的性能。
我们提供了大量的实验来证明它对退化图像质量和校准误差的鲁棒性。将该方法推广到3D跟踪任务中，获得nuScenes跟踪排行榜第一名，显示了该方法的有效性和泛化能力。
#### 动机
- 由于点云的稀疏性，单靠激光雷达的方法不足以实现鲁棒的三维探测。例如，在激光雷达模式下，小的或遥远的物体很难被探测到。相比之下，这些物体在高分辨率的图像中仍然清晰可见。点云和图像的互补作用促使研究人员设计利用这两个世界的最好的检测器，即多模态检测器。
- 现有的激光雷达-摄像机融合方法大致分为三类:result-level, proposal-level, 和pointlevel。结果级方法：使用现成的2D检测器来生成3D建议，然后使用PointNet[30]来进行对象定位。包括MV3D[5]和AVOD[12]在内的提案级融合方法，在区域提案级对共享提案的每一种模式应用RoIPool[31]进行融合。由于矩形感兴趣区域通常含有大量的背景噪声，这些粗粒度融合方法的结果并不理想。近年来，许多方法都尝试进行点级融合，并取得了良好的效果。他们首先基于校准矩阵找到LiDAR点与图像像素之间的硬关联，然后通过逐点拼接，用关联像素的分割分数[47,52]或CNN特征[10,22,40,48,63]来增强LiDAR特征。类似地，[16,17,51,60]首先将点云投影到bird’s eye view (BEV)平面上，然后将图像特征与BEV像素融合。
#### 动机
- 尽管这些改进令人印象深刻，但这些点级融合方法存在两个主要问题，如图1所示。首先，它们只是通过逐元素的添加或拼接将LiDAR特征与图像特征融合在一起，当图像特征质量较低时(如光照条件较差的图像)，其性能会严重下降。其次，寻找稀疏的LiDAR点与密集的图像像素之间的硬关联，不仅浪费了大量具有丰富语义信息的图像特征，而且严重依赖于两个传感器之间的高质量校准，而由于固有的时空偏差，这种校准通常很难获得。重要：LIF-Seg: LiDAR and Camera Image Fusion for 3D LiDAR Semantic Segmentation

#### 创新点
我们的关键思想是重新定位融合过程的重点，从硬关联到软关联，以导致鲁棒性退化图像质量和传感器不对准。

